# Retrieval API

This project is a production-ready Retrieval system. It integrates LLMs (OpenAI), vector databases, and custom retrievers to deliver context-aware chat responses.

## Features
- Vector-based retrieval with support for custom retrievers, & Vectorstores
- Conversational memory (PostgreSQL-backed)
- FastAPI backend with CORS support


## ğŸ“ Folder Structure
```bash
â”œâ”€â”€ main.py                         # FastAPI entry point (app, CORS, routes)
â”œâ”€â”€ .env                            # Environment variables (OpenAI key, DB credentials)
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml                 # YAML config for chain types, embeddings, vector DB, memory, etc.
â”‚
â”œâ”€â”€ app_utils/
â”‚   â””â”€â”€ config_loader.py           # Loads and parses YAML configuration
    â””â”€â”€ logger.py                  # App Logging configuration
    â”œâ”€â”€ app.log                    # Stores all the application Log
â”‚
â”œâ”€â”€ llms/
â”‚   â”œâ”€â”€ openai_llm.py              # OpenAI model wrapper
    â””â”€â”€ base_llm.py                # Abstract base for LLM
â”‚   â””â”€â”€ llm_factory.py             # Initializes LLM based on config
â”‚
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ openai_embedding.py        # OpenAI embedding generator
â”‚   â””â”€â”€ embedding_factory.py       # Factory to load the correct embedding model
â”‚
â”œâ”€â”€ vectorstore/
â”‚   â”œâ”€â”€ pgvector_store.py          # Postgres DB wrapper
â”‚   â””â”€â”€ vectorstore_factory.py     # Chooses and initializes the vector store
â”‚
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ vector_retriever.py        # Builds retriever from vector DB.
â”‚   â””â”€â”€ retriever_factory.py       # Factory to load retriever based on config.
    â””â”€â”€ multiquery_retriever.py    # wrapper of LangChain Mult-Query Retriever.
â”‚
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ base_memory.py             # Abstract base class for memory
â”‚   â”œâ”€â”€ postgres_memory.py         # PostgreSQL-based memory with session_id
â”‚   â””â”€â”€ memory_factory.py          # Chooses memory backend (Postgres, Redis, etc.)
â”‚
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ conversational_prompt.py   # Prompt templates used in ConversationalChain
â”‚
â”œâ”€â”€ chains/
â”‚   â”œâ”€â”€ conversational_chain.py    # Constructs ConversationalRetrievalChain
â”‚   â””â”€â”€ chain_factory.py           # Chooses chain type (currently conversational)
â”‚
â”œâ”€â”€ services/
â”‚   â””â”€â”€ chat_service.py            # Core logic: takes session_id + question and returns answer + source files
â”‚
â”œâ”€â”€ api/v1
â”‚   â””â”€â”€ chat_routes.py             # FastAPI router that defines POST /chat endpoint

```
### 1. Clone the repository
```bash
   git clone https://github.com/spiredata/Arkaa_Retrieval.git
   cd retrieval-api
```
### 2. Run the API
```bash
uvicorn main:app --reload
```

# ğŸ“˜ API Documentation

## ğŸ”— Endpoint: `/chat`
Handles user queries via Retrieval-Augmented Generation (RAG) using LangChain and returns the generated answer along with source file paths.

### Method
`POST`

### Request Body (JSON)
```bash
{
  "session_id": "string",     // Unique session identifier (e.g., user ID or UUID)
  "question": "string"        // User's query to the system
}
```
### Response Body (JSON)
```bash
{
  "answer": "string",         // Answer generated by the chain
  "source_documents": [       // Unique list of file paths from which relevant chunks were retrieved
    "path/to/file1.pdf",
    "path/to/file2.pdf"
  ]
}
```
