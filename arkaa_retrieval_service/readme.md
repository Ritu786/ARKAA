# Retrieval API

This project is a production-ready Retrieval system. It integrates LLMs (OpenAI), vector databases, and custom retrievers to deliver context-aware chat responses.

## Features
- Vector-based retrieval with support for custom retrievers, & Vectorstores
- Conversational memory (PostgreSQL-backed)
- FastAPI backend with CORS support


## 📁 Folder Structure
```bash
├── main.py                         # FastAPI entry point (app, CORS, routes)
├── .env                            # Environment variables (OpenAI key, DB credentials)
│
├── config/
│   └── config.yaml                 # YAML config for chain types, embeddings, vector DB, memory, etc.
│
├── app_utils/
│   └── config_loader.py           # Loads and parses YAML configuration
    └── logger.py                  # App Logging configuration
    ├── app.log                    # Stores all the application Log
│
├── llms/
│   ├── openai_llm.py              # OpenAI model wrapper
    └── base_llm.py                # Abstract base for LLM
│   └── llm_factory.py             # Initializes LLM based on config
│
├── embeddings/
│   ├── openai_embedding.py        # OpenAI embedding generator
│   └── embedding_factory.py       # Factory to load the correct embedding model
│
├── vectorstore/
│   ├── pgvector_store.py          # Postgres DB wrapper
│   └── vectorstore_factory.py     # Chooses and initializes the vector store
│
├── retrieval/
│   ├── vector_retriever.py        # Builds retriever from vector DB.
│   └── retriever_factory.py       # Factory to load retriever based on config.
    └── multiquery_retriever.py    # wrapper of LangChain Mult-Query Retriever.
│
├── memory/
│   ├── base_memory.py             # Abstract base class for memory
│   ├── postgres_memory.py         # PostgreSQL-based memory with session_id
│   └── memory_factory.py          # Chooses memory backend (Postgres, Redis, etc.)
│
├── prompts/
│   └── conversational_prompt.py   # Prompt templates used in ConversationalChain
│
├── chains/
│   ├── conversational_chain.py    # Constructs ConversationalRetrievalChain
│   └── chain_factory.py           # Chooses chain type (currently conversational)
│
├── services/
│   └── chat_service.py            # Core logic: takes session_id + question and returns answer + source files
│
├── api/v1
│   └── chat_routes.py             # FastAPI router that defines POST /chat endpoint

```
### 1. Clone the repository
```bash
   git clone https://github.com/spiredata/Arkaa_Retrieval.git
   cd retrieval-api
```
### 2. Run the API
```bash
uvicorn main:app --reload
```

# 📘 API Documentation

## 🔗 Endpoint: `/chat`
Handles user queries via Retrieval-Augmented Generation (RAG) using LangChain and returns the generated answer along with source file paths.

### Method
`POST`

### Request Body (JSON)
```bash
{
  "session_id": "string",     // Unique session identifier (e.g., user ID or UUID)
  "question": "string"        // User's query to the system
}
```
### Response Body (JSON)
```bash
{
  "answer": "string",         // Answer generated by the chain
  "source_documents": [       // Unique list of file paths from which relevant chunks were retrieved
    "path/to/file1.pdf",
    "path/to/file2.pdf"
  ]
}
```
