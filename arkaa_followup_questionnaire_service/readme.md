Follow-Up Question Generation API

A FastAPI base microservice for genrating context aware follow-up questions from document chunks using OpenAI's GPt models.
Desgined with Async Processing and modular architecture for scalability. 

ğŸš€ Features

1. ğŸ”— LLM-Powered: Uses OpenAI's gpt-4o for high-quality question generation.
2. âš¡ Asynchronous: Fully async with retries and exponential backoff.
3. ğŸ“„ Document Chunking: Handles multiple text chunks in a single session.
4. ğŸ“ˆ Structured Logging: Context-aware logs for every session.
5. ğŸ“¦ Modular Design: Clean separation of concerns (routing, services, utils).

ğŸ“ Project Structure
```bash
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ v01/
â”‚   â”‚       â”œâ”€â”€ routes/
â”‚   â”‚       â”‚   â””â”€â”€ followup_routes.py
â”‚   â”‚       â”œâ”€â”€ services/
â”‚   â”‚       â”‚   â”œâ”€â”€ qa_service.py
â”‚   â”‚       â”‚   â”œâ”€â”€ followup_manager.py
â”‚   â”‚       â”‚   â””â”€â”€ chunk_processor_service.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ main.py
â”œâ”€â”€ .env
â”œâ”€â”€ config.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

âš™ï¸ Setup

1. Clone the repo
```bash
git clone <Repo URI>
cd app
```

2. Set environment variables
Create a .env fiel
```bash
OPENAI_API_KEY = 'your-openai-key'
```

5. Run the Service
```bash
uvicorn app_main:app --reload
```

ğŸ§ª API Usage

Endpoint
```bash
POST /api/v01/generate-followups
```

Request Body
```bash
{
  "session_id": "string",
  "current_question": "string",
  "source_documents": 
  [
    {
      "metadata": {
        "additionalProp1": {}
      },
      "page_content": "string"
    }
  ]
}
```

Response
```bash
{
  "session_id": "string",
  "questions": [
    {
      "question": "string",
      "answer": "string"
    }
  ]
}
```
ğŸ› ï¸ Developer Notes
1. Uses "tenacity" for retry logic.
2. Uses "tiktoken" for monitoring token activity.
3. Utilized Async OpenAI for generating Q&A.
4. All routes and services have built-in logging with session context.

ğŸ§­ Future Improvements
1. ğŸ”„ Support for stream=True with proper async yield for real-time UX.
2. ğŸ§ª Unit and integration test coverage.

